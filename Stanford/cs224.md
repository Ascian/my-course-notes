# CS224

- [Word Vectors](#word-vectors)
  - [Word meaning](#word-meaning)
  - [Word2vec introduction](#word2vec-introduction)
  - [Word2vec objective function gradients](#word2vec-objective-function-gradients)
  - [Optimization refresher](#optimization-refresher)
- [A Deep Look at Word Vectors](#a-deep-look-at-word-vectors)
  - [The skip-gram model and negative sampling](#the-skip-gram-model-and-negative-sampling)


## Word Vectors

### Word meaning

**Problems with discrete representation(like atomic symbols:hotel,conference,etc)**

* missing nuances(细微差别)
* hard to compute accurate word similarity
* doesn't give any inherent(固有的) notion(概念，主张) of relationships between words 

**One-hot representation**

A vector with one 1 and a lot of zeroes to represents a word

**Distributional similarity(分布相似性)**

You can get a lot of value by representing a word by means of its neighbors

**Distributed representations(分布式表示)**

Build a dense(稠密) vector for each word type，chosen so that it is good at prediction other words appearing in its context

### Word2vec introduction

Predict between every word and its context words

#### Skip-gram(跳过语法) prediction: for producing word vectors

For each word t = 1 ... T, predict surrounding words in a window of "radius"m of every word

Objective function: maximize the probability of any context word given the current center word

$$J'(\theta)=\prod \limits_{t=1}^T \prod \limits_{-m \leq j \leq m} P(w_{t+j}|w_t, \theta)$$

its negative log likelihood version:

$$J(\theta)=- \frac{1}{T} \sum \limits_{t=1}^T \sum \limits_{-m \leq j \leq m} log P(w_{t+j}|w_t, \theta)$$

for $P(w_{t+j}|w_t, \theta)$ the simplest first formulation is

$$P(o|c) = \frac{exp(u_o · v_c)}{\sum \limits_{w=1}^{v}exp(u_w · v_c)}$$

$\sum \limits_{w=1}^{v}exp(u_w · v_c)$ is the sum of all words besides the words in the window, that means P(o|c) is the probability that word o is the word occurs within the window at any position given the center word of c

two vectors for each word, $v_c$ and $u_o$ are "center" and "outside" vectors of indices c and o

>**$u · v$(点乘) is bigger if u and v are more similar**, this is a measure of similarity between vectors

>Softmax function:Standard way to turn numbers into probability distribution:
> $$p_i=\frac{e^{u_i}}{\sum_j e^{u_j}}$$
> $e^{u_i}$ to make positive, $\frac{e^{u_i}}{\sum_j e^{u_j}}$to normalize to give probability

With d-dimensional vector and V many words, we define the set of all parameters in a model in terms of one long vector $\theta$:

$$\theta = \left[
    \begin{array}{l}
    v_{aardvark} \\
    v_a \\
    ... \\
    v_{zebra} \\
    u_{aardvark} \\
    u_a \\
    ... \\
    u_{zebra}
    \end{array}
    \right] \in R^{2dV}$$





### Word2vec objective function gradients

>当 P 沿着 l 趋于 P$_0$时极限存在，则称此极限为函数 f(x,y) 在点 P$_0$ 沿方向 l 的方向导数，记作$\left.\frac{\partial f}{\partial l} \right| _{(x_0,y_0)}$，即
>$$\left.\frac{\partial f}{\partial l} \right| _{(x_0,y_0)} = \lim_{x \to 0^+} \frac{f(x_0 + \rho cos\alpha, y_0 + \rho cos \beta) - f(x_0, y_0)}{\rho} = f_x(x_0, y_0)cos\alpha + f_y(x_0, y_0) cos\beta$$

> 对任意点 P, 都可以定义一个向量$f_x(x_0, y_0)i + f_y(x_0, y_0)j$，该向量称为函数f(x, y)在点 P 的梯度，记作 $grad f(x_0, y_0)$或$\nabla f$
> 它的方向时函数在该点方向导数取得最大值的方向，它的模就等于方向导数的最大值。同时他的反方向就是函数减少最快的方向

To compute the gradient of $J(\theta)$，we should work out  every partial derivatives(导数) of $J(\theta)$ with respect to the vectors. Firstly ,we should work out the partial derivatives with respect to the center vector.

$$\begin{array}{l}
\frac{\partial}{\partial v_c}log \frac{exp(u_o^Tv_c)}{\sum \limits_{w=1}^v exp(u_w^Tv_c)} \\
= \frac{\partial}{\partial v_c}log exp(u_o^Tv_c) - \frac{\partial}{\partial v_c}log \sum \limits_{w=1}^v exp(u_w^Tv_c) \\
= u_o - \frac{1}{\sum \limits_{w=1}^v exp(u_w^Tv_c)}\sum \limits_{x=1}^v \left[exp(u_x^Tv_c)u_x\right] \\
= u_o - \sum \limits_{x=1}^v \frac{exp(u_x^Tv_c)}{\sum \limits_{w=1}^v exp(u_w^Tv_c)}u_x \\
=u_o - \sum \limits_{x=1}^v p(x|c) u_x
\end{array}$$

### Optimization refresher

To minimize $J(\theta)$, we are trying to use gradient descent(梯度下降)

> gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest(最急剧的) descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function

With step size $\alpha$, update $\theta$ by

$$\theta ^{new} = \theta^{old} - \alpha \frac{\partial}{\partial \theta} J(\theta)$$

But this algorithm requires a big amount of computation. So we use stochastic(随机) gradient descent(SGD) instead.

Instead of using the gradient $\frac{\partial}{\partial \theta} J(\theta)$ computed with all the windows, we just compute $\frac{\partial}{\partial \theta} J_t(\theta)$ in one window t, and using that estimate of the gradient in that window to update the argument:

$$\theta ^{new} = \theta^{old} - \alpha \frac{\partial}{\partial \theta} J_t(\theta)$$

this estimate of gradient is incredibly noisy, but in fact, it works better and faster.

However, $\sum \limits_{w=1}^v exp(u_w^Tv_c)$ needs to go through all the words of the corpus, that still needs a big amount of computation

## A Deep Look at Word Vectors

### The skip-gram model and negative sampling

overall objective function: $J(\theta)=\frac{1}{T}\sum_{t=1}^T J_t(\theta)$

$$J_t(\theta)=log\sigma(u_o^Tv_c) + \sum \limits_{j \sim P(w)}[log\sigma(-u_j^T v_c)]$$

> $\sigma$指Sigmoid函数，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的激活函数，将变量映射到0, 1之间。可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。
> $$S(x)=\frac{1}{1 + e^{-x}}$$
> ![Sigmoid 曲线](img/Sigmoid%E6%9B%B2%E7%BA%BF.png)

* Sigmoid function can squashes(压缩) any real number to be between 0 and 1, it is good enough to call it a probability, $log\sigma(u_o^Tv_c)$ means log probability of two words co-occurring
* Instead of going through all the words of the corpus, we subsample a couple of random words from corpus(语料库) to reduce computation. $\sum \limits_{j \sim P(w)}[log\sigma(-u_j^T v_c)]$ means the sum of log probability that the randomly sampled word and the center word are not co-occurring

The goal is to maximize probability that real outside word appears and minimize probability that random words appear around center word.

We use the unigram distribution(一元分布) to define $p(w)$:

$$P(w)=U(w)^{3/4}/Z=\frac{[count(w)^{3/4}]}{\sum_{u \subset D}[count(u)]^{3/4}}$$

> A **unigram distribution** is a probability distribution over a set of discrete items, where each item represents a single element (or "unigram") in a sequence of elements. In natural language processing, unigrams are often used to model the probability of individual words in a text corpus. The unigram distribution can be estimated from a sample of text by counting the number of occurrences of each word and normalizing the counts to obtain probabilities. These probabilities can then be used to predict the likelihood of a given word appearing in a new text sample.

The 3/4 power makes less frequent words be samples more often.