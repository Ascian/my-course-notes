# CS224

- [Word Vectors](#word-vectors)
  - [Word meaning](#word-meaning)
  - [Word2vec introduction](#word2vec-introduction)
  - [Word2vec objective function gradients](#word2vec-objective-function-gradients)
  - [Optimization refresher](#optimization-refresher)


## Word Vectors

### Word meaning

**Problems with discrete representation(like atomic symbols:hotel,conference,etc)**

* missing nuances(细微差别)
* hard to compute accurate word similarity
* doesn't give any inherent(固有的) notion(概念，主张) of relationships between words 

**One-hot representation**

A vector with one 1 and a lot of zeroes to represents a word

**Distributional similarity(分布相似性)**

You can get a lot of value by representing a word by means of its neighbors

**Distributed representations(分布式表示)**

Build a dense(稠密) vector for each word type，chosen so that it is good at prediction other words appearing in its context

### Word2vec introduction

Predict between every word and its context words

#### Skip-gram(跳过语法) prediction: for producing word vectors

For each word t = 1 ... T, predict surrounding words in a window of "radius"m of every word

Objective function: maximize the probability of any context word given the current center word

$$J'(\theta)=\prod \limits_{t=1}^T \prod \limits_{-m \leq j \leq m} P(w_{t+j}|w_t, \theta)$$

its negative log likelihood version:

$$J(\theta)=- \frac{1}{T} \sum \limits_{t=1}^T \sum \limits_{-m \leq j \leq m} log P(w_{t+j}|w_t, \theta)$$

for $P(w_{t+j}|w_t, \theta)$ the simplest first formulation is

$$P(o|c) = \frac{exp(u_o · v_c)}{\sum \limits_{w=1}^{v}exp(u_w · v_c)}$$

$\sum \limits_{w=1}^{v}exp(u_w · v_c)$ is the sum of all words besides the words in the window, that means P(o|c) is the probability that word o is the word occurs within the window at any position given the center word of c

two vectors for each word, $v_c$ and $u_o$ are "center" and "outside" vectors of indices c and o

>**$u · v$(点乘) is bigger if u and v are more similar**, this is a measure of similarity between vectors

>Softmax function:Standard way to turn numbers into probability distribution:
> $$p_i=\frac{e^{u_i}}{\sum_j e^{u_j}}$$
> $e^{u_i}$ to make positive, $\frac{e^{u_i}}{\sum_j e^{u_j}}$to normalize to give probability

With d-dimensional vector and V many words, we define the set of all parameters in a model in terms of one long vector $\theta$:

$$\theta = \left[
    \begin{array}{l}
    v_{aardvark} \\
    v_a \\
    ... \\
    v_{zebra} \\
    u_{aardvark} \\
    u_a \\
    ... \\
    u_{zebra}
    \end{array}
    \right] \in R^{2dV}$$





### Word2vec objective function gradients

>当 P 沿着 l 趋于 P$_0$时极限存在，则称此极限为函数 f(x,y) 在点 P$_0$ 沿方向 l 的方向导数，记作$\left.\frac{\partial f}{\partial l} \right| _{(x_0,y_0)}$，即
>$$\left.\frac{\partial f}{\partial l} \right| _{(x_0,y_0)} = \lim_{x \to 0^+} \frac{f(x_0 + \rho cos\alpha, y_0 + \rho cos \beta) - f(x_0, y_0)}{\rho} = f_x(x_0, y_0)cos\alpha + f_y(x_0, y_0) cos\beta$$

> 对任意点 P, 都可以定义一个向量$f_x(x_0, y_0)i + f_y(x_0, y_0)j$，该向量称为函数f(x, y)在点 P 的梯度，记作 $grad f(x_0, y_0)$或$\nabla f$
> 它的方向时函数在该点方向导数取得最大值的方向，它的模就等于方向导数的最大值。同时他的反方向就是函数减少最快的方向

To compute the gradient of $J(\theta)$，we should work out  every partial derivatives(导数) of $J(\theta)$ with respect to the vectors. Firstly ,we should work out the partial derivatives with respect to the center vector.

$$\begin{array}{l}
\frac{\partial}{\partial v_c}log \frac{exp(u_o^Tv_c)}{\sum \limits_{w=1}^v exp(u_w^Tv_c)} \\
= \frac{\partial}{\partial v_c}log exp(u_o^Tv_c) - \frac{\partial}{\partial v_c}log \sum \limits_{w=1}^v exp(u_w^Tv_c) \\
= u_o - \frac{1}{\sum \limits_{w=1}^v exp(u_w^Tv_c)}\sum \limits_{x=1}^v \left[exp(u_x^Tv_c)u_x\right] \\
= u_o - \sum \limits_{x=1}^v \frac{exp(u_x^Tv_c)}{\sum \limits_{w=1}^v exp(u_w^Tv_c)}u_x \\
=u_o - \sum \limits_{x=1}^v p(x|c) u_x
\end{array}$$

### Optimization refresher

To minimize $J(\theta)$, we are trying to use gradient descent(梯度下降)

> gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest(最急剧的) descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function

With step size $\alpha$, update $\theta$ by

$$\theta ^{new} = \theta^{old} - \alpha \frac{\partial}{\partial \theta} J(\theta)$$

But this algorithm requires a big amount of computation. So we use stochastic(随机) gradient descent(SGD) instead.

Instead of using the gradient $\frac{\partial}{\partial \theta} J(\theta)$ computed with all the windows, we just compute $\frac{\partial}{\partial \theta} J_t(\theta)$ in one window t, and using that estimate of the gradient in that window to update the argument:

$$\theta ^{new} = \theta^{old} - \alpha \frac{\partial}{\partial \theta} J_t(\theta)$$

this estimate of gradient is incredibly noisy, but in fact, it works better and faster.