# Attention Mechanism

## Motivation

Compare this to databases. In their simplest form they are collections of keys (𝑘) and values (𝑣). For instance, our database D might consist of tuples {(“Zhang”, “Aston”), (“Lipton”, “Zachary”), (“Li”, “Mu”), (“Smola”, “Alex”), (“Hu”, “Rachel”), (“Werness”, “Brent”)} with the last name being the key and the first name being the value. We can operate on D, for instance with the exact query (𝑞) for “Li” which would return the value “Mu”. If (“Li”, “Mu”) was not a record in D, there would be no valid answer. If we also allowed for approximate matches, we would retrieve (“Lipton”, “Zachary”) instead. This quite simple and trivial example nonetheless teaches us a number of useful things: 

* We can design queries 𝑞 that operate on (𝑘,𝑣) pairs in such a manner as to be valid regardless of the database size.  
* The same query can receive different answers, according to the contents of the database. 
*  There is no need to compress or simplify the database to make the operations effective.

## Introduction 

In the case of a (scalar) **regression** with observations $x_𝑖$, $𝑦_𝑖$ for features and labels respectively, $v_𝑖 = 𝑦_𝑖$ are scalars, $k_𝑖 = x_𝑖$ are vectors, and the query $q$ denotes the new location where $𝑓$ should be evaluated.

 In the case of (multiclass) **classification**, we use one-hotencoding of $𝑦_𝑖$ to obtain $v_𝑖$.

In these two cases, it can be used directly with little to no training or tuning.

In the case of **words sepuence**, each time the proposed model generates a word in a translation, it searches($softmax(q \cdot k)$)  for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.[(Bahdanau et al., 2014)](../../../.Scholar/storage/AL53GHFN/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf)

## Formula

$Attention \overset{def}{=} \sum\limits^{m}_{i=1} \alpha(q, k_i)v_i$

Attention weight $\alpha(q,k_i)=\frac{exp(a(q,k_i))}{\sum_jexp(a(q,k_i))}$

### Scaled Dot Product Attention

Gaussian kernel: $$a(q,k_i)=-\frac{1}{2}||q-k_i||^2=q^Tk_i-\frac{1}{2}||k_i||^2-\frac{1}{2}||q||^2$$

* $\frac{1}{2}||q||^2$ depends on q only. Normalizing the attention weights to 1, as is done by *software function*, ensures that this term disappears entirely.
* If the $k_i$ is generated by a layer norm or a batch norm, the $||k_i||$ will be well-bounded. As such, we can drop it from the definition of 𝑎 without any major change in the outcome.
* Assume that all the elements of the query $q$  and the key $k_𝑖$ are with zero mean and unit variance. The dot product between both vectors would have zero mean and a variance of 𝑑.  We suspect that for large values of $d$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Thus, we rescale the dot product by $\frac{1}{\sqrt{d}}$ 

These leads to the new attention formula:

$$a(q,k_i)=\frac{q^Tk_i}{\sqrt{d}}$$

### Additive Attention

When queries q and keys k are vectors of **different dimension**, we can either use a matrix to address the mismatch via $q^TMk$, or we can use additive attention as the scoring function.

For additive attention, the query and key are concatenated and fed into an MLP with a single hidden layer.

$$a(q,k)=w^T_ttanh(W_qq+W_kk)$$

## Advantage

* Intuitive: we might interpret large weights as a way for the model to select components of relevance
* Differentiable means of control



## Inspiration

It's hard to find out which word is more relavent or whose information is more important, then create a mechanism to make the machine to learn it.