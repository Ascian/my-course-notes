# Attention Mechanism

## Motivation

Compare this to databases. In their simplest form they are collections of keys (ğ‘˜) and values (ğ‘£). For instance, our database D might consist of tuples {(â€œZhangâ€, â€œAstonâ€), (â€œLiptonâ€, â€œZacharyâ€), (â€œLiâ€, â€œMuâ€), (â€œSmolaâ€, â€œAlexâ€), (â€œHuâ€, â€œRachelâ€), (â€œWernessâ€, â€œBrentâ€)} with the last name being the key and the first name being the value. We can operate on D, for instance with the exact query (ğ‘) for â€œLiâ€ which would return the value â€œMuâ€. If (â€œLiâ€, â€œMuâ€) was not a record in D, there would be no valid answer. If we also allowed for approximate matches, we would retrieve (â€œLiptonâ€, â€œZacharyâ€) instead. This quite simple and trivial example nonetheless teaches us a number of useful things: 

* We can design queries ğ‘ that operate on (ğ‘˜,ğ‘£) pairs in such a manner as to be valid regardless of the database size.  
* The same query can receive different answers, according to the contents of the database. 
*  There is no need to compress or simplify the database to make the operations effective.

## Introduction 

In the case of a (scalar) **regression** with observations $x_ğ‘–$, $ğ‘¦_ğ‘–$ for features and labels respectively, $v_ğ‘– = ğ‘¦_ğ‘–$ are scalars, $k_ğ‘– = x_ğ‘–$ are vectors, and the query $q$ denotes the new location where $ğ‘“$ should be evaluated.

 In the case of (multiclass) **classification**, we use one-hotencoding of $ğ‘¦_ğ‘–$ to obtain $v_ğ‘–$.

In these two cases, it can be used directly with little to no training or tuning.

In the case of **words sepuence**, each time the proposed model generates a word in a translation, it searches($softmax(q \cdot k)$)  for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.[(Bahdanau et al., 2014)](../../../.Scholar/storage/AL53GHFN/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf)

## Formula

$Attention \overset{def}{=} \sum\limits^{m}_{i=1} \alpha(q, k_i)v_i$

Attention weight $\alpha(q,k_i)=\frac{exp(a(q,k_i))}{\sum_jexp(a(q,k_i))}$

### Scaled Dot Product Attention

Gaussian kernel: $$a(q,k_i)=-\frac{1}{2}||q-k_i||^2=q^Tk_i-\frac{1}{2}||k_i||^2-\frac{1}{2}||q||^2$$

* $\frac{1}{2}||q||^2$ depends on q only. Normalizing the attention weights to 1, as is done by *software function*, ensures that this term disappears entirely.
* If the $k_i$ is generated by a layer norm or a batch norm, the $||k_i||$ will be well-bounded. As such, we can drop it from the definition of ğ‘ without any major change in the outcome.
* Assume that all the elements of the query $q$  and the key $k_ğ‘–$ are with zero mean and unit variance. The dot product between both vectors would have zero mean and a variance of ğ‘‘.  We suspect that for large values of $d$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Thus, we rescale the dot product by $\frac{1}{\sqrt{d}}$ 

These leads to the new attention formula:

$$a(q,k_i)=\frac{q^Tk_i}{\sqrt{d}}$$

### Additive Attention

When queries q and keys k are vectors of **different dimension**, we can either use a matrix to address the mismatch via $q^TMk$, or we can use additive attention as the scoring function.

For additive attention, the query and key are concatenated and fed into an MLP with a single hidden layer.

$$a(q,k)=w^T_ttanh(W_qq+W_kk)$$

## Advantage

* Intuitive: we might interpret large weights as a way for the model to select components of relevance
* Differentiable means of control



## Inspiration

It's hard to find out which word is more relavent or whose information is more important, then create a mechanism to make the machine to learn it.